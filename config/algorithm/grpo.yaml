name: GRPO

args:

  # GRPO Config
  max_prompt_length: ${task.args.max_prompt_length}
  num_generations: 8
  max_completion_length: ${task.args.max_completion_length}
  shuffle_dataset: false
  steps_per_generation: 1
  use_vllm: true
  vllm_mode: server
  beta: 0.001
  loss_type: grpo
  log_completions: true
  wandb_log_unique_prompts: true

  # Trainer Config
  accelerator_config:
    split_batches: true
  bf16: true
  ddp_find_unused_parameters: false
  eval_steps: 400
  eval_strategy: steps
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  learning_rate: 1e-06
  log_on_each_node: false
  logging_steps: 10
  logging_strategy: steps
  lr_scheduler_type: cosine
  max_steps: ${task.args.max_steps}
  overwrite_output_dir: true
  per_device_train_batch_size: 16
  report_to:
    - tensorboard
    # - wandb
  save_strategy: "no"
  seed: 42
  tf32: true
  output_dir: outputs/${model.name}_${task.name}_${algorithm.e2h_args.curriculum_schedule}
  logging_dir: ${algorithm.args.output_dir}

e2h_args:
  curriculum_schedule: "cosine" # cosine, gaussian, balanced (GRPO), classic (traditional curriculum learning)
  max_dapo_iter: 8
  scheduler_args:
    mu_exp: 0.5 # If using Gaussian schedule, this is the mean
    sigma: 0.5 # If using Gaussian schedule, this is the standard deviation
    min_prob: true # Whether to use min probability for sampling tasks
    beta: 1.0