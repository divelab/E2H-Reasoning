# conf/algorithm/grpo.yaml
name: "sgrpo"

training:
  learning_rate: 1e-6
  lr_scheduler_type: "cosine"
  logging_steps: 10
  max_steps: 300
  curriculum: true
  curriculum_schedule: "gaussian" # balanced in default, gaussian, cosine or classic
  scheduler_params:
    mu_exp: 0.5
    sigma: 0.5
    min_prob: true    # true: use default p_min. (float): use min_prob value. false: use 0.0
  per_device_train_batch_size: 1  
  gradient_accumulation_steps: 4
  gradient_checkpointing: true
  bf16: true
  num_generations: 8
  beta: 0.001
  use_vllm: true
  vllm_gpu_memory_utilization: 0.2
  report_to:
    - wandb
  push_to_hub: true
  save_strategy: "steps"
  save_steps: ${algorithm.training.max_steps}
  eval_strategy: "steps"
  tf32: true