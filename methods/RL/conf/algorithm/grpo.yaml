# conf/algorithm/grpo.yaml
name: "grpo"

training:
  resume_from_checkpoint: null
  learning_rate: 1e-6
  lr_scheduler_type: "cosine"
  logging_steps: 10
  max_steps: 1600
  per_device_train_batch_size: 16 # completion number in one mini step
  generation_batch_size: null # completion number in one mini step
  steps_per_generation: 1 # generation_batch_size = steps_per_generation * per_device_train_batch_size * num_gpus but its default is gradient_accumulation_steps, seems not right.
  gradient_accumulation_steps: 4 # 4 mini steps one global step
  gradient_checkpointing: true
  bf16: true
  report_to:
    - wandb
  push_to_hub: true
  save_strategy: "steps"
  save_steps: ${algorithm.training.max_steps}
  tf32: true
  num_generations: 8
  beta: 0.001
  use_vllm: true
  vllm_mode: "server"  # Use colocated vLLM instead of server mode
  vllm_gpu_memory_utilization: 0.3
  vllm_server_port: 8000
  curriculum: false
  curriculum_schedule: "balanced" # fixed in default # "balanced", "gaussian", "vrex"
  scheduler_params:
    mu_exp: 0.5
    sigma: 0.5
    vrex_adds:
      groupdro: 1.0
      gaussian: 0.0
      sec: 0.3  # SEC (Self-Evolving Curriculum) weight
    beta: 1.0
    min_prob: true    # true: use default p_min. (float): use min_prob value. false: use 0.0
    # SEC (Self-Evolving Curriculum) parameters - defaults from paper
    td_alpha: 0.5      # TD learning rate (mostly 0.5 across tasks in paper)
    sec_temperature: 0.3  # Boltzmann temperature (mostly 1.0 for 3B model)
    max_dapo_iter: 4