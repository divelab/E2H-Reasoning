# CLAUDE.md

This file provides guidance for working with the Easy 2 Hard (E2H) Reasoner implementation in Sys2Bench.

## Table of Contents

1. [Overview](#overview)
2. [Paper Implementation](#paper-implementation)
3. [Key Commands](#key-commands)
4. [Architecture](#architecture)
5. [Curriculum Schedulers](#curriculum-schedulers)
6. [Supported Tasks](#supported-tasks)
7. [Configuration](#configuration)
8. [Running Experiments](#running-experiments)

## Overview

This repository contains the implementation of the paper "Curriculum Reinforcement Learning from Easy to Hard Tasks Improves LLM Reasoning" (https://arxiv.org/html/2506.06632v1). The E2H Reasoner uses curriculum reinforcement learning to improve language models' reasoning capabilities by training them on tasks with progressively increasing difficulty.

## Must read

- When you read this, read my paper carefully: https://arxiv.org/html/2506.06632v1
- Read methods/RL/CLAUDE.md

## Paper Implementation

The entire implementation is contained in `/methods/RL/main.py`, which serves as the only valid entry point. This implementation includes:

- **Task Sampling**: Custom `TaskSampler` class that implements curriculum scheduling
- **Curriculum Schedulers**: Multiple scheduling strategies for task difficulty progression
- **GRPO Training**: Modified `CurriculumGRPOTrainer` that integrates with the task sampling
- **Multi-task Support**: Training across different difficulty levels within each task

## Key Commands

### Setup and Environment
```bash
# Activate environment
conda activate sys2bench
```

### Training with E2H

Using the new `run_train.py` wrapper script (recommended):
```bash
python run_train.py cuda_visible_devices=0,1 --num_processes 1 --main_process_port 29750 --config_file methods/RL/deep_speed.yaml methods/RL/main.py mode=train task=countdown2345 algorithm=grpo algorithm.training.curriculum_schedule=cosine model=qwen15 algorithm.training.per_device_train_batch_size=2 algorithm.training.scheduler_params.mu_exp=0.5 algorithm.training.scheduler_params.sigma=0.5 algorithm.training.max_steps=1600
```

Or using the original command:
```bash
WANDB_PROJECT=Sys2Bench ROOT_PATH=/data/shurui.gui/Projects/Sys2Bench CUDA_VISIBLE_DEVICES=0,1 accelerate launch --num_processes 1 --main_process_port=29750 --config_file methods/RL/deep_speed.yaml  methods/RL/main.py mode=train task=countdown2345 algorithm=grpo algorithm.training.curriculum_schedule=cosine model=qwen15 algorithm.training.per_device_train_batch_size=2 algorithm.training.scheduler_params.mu_exp=0.5 algorithm.training.scheduler_params.sigma=0.5 algorithm.training.max_steps=1600
```
- cuda_visible_devices (or CUDA_VISIBLE_DEVICES): Specify GPUs to use (e.g., 0,1 for two GPUs). The last gpu is always reserved for VLLM. So you can see that `--num_processes 1`, when num_processes is set to 1, it will use the first GPU for model training and the last GPU for VLLM.
- countdown2345: path methods/RL/tasks/countdown2345, which contains the data files for countdown tasks with different difficulty levels (2 numbers, 3 numbers, 4 numbers, and 5 numbers). Schedulers will sample these 4-level tasks.
- algorithm: we only use grpo.
- curriculum_schedule: the curriculum scheduler to use. Options are `balanced`, `classic`, `cosine`, `gaussian`, and `variance_regularized` (vrex is being exploring). The default is `balanced`.
- model: the model to use. We use 1.5B and 3B Qwen models, but we mainly use 1.5B for research exploration, i.e., model=qwen15.
- algorithm.training.per_device_train_batch_size=2: that is 2 samples per GPU, and since we use 1 GPU (not considering the vllm GPU), our batch size is 2 * 4 (gradient_accumulation_steps: 4 shown in the config methods/RL/conf/algorithm/grpo.yaml) = 8.
- algorithm.training.scheduler_params.mu_exp=0.5 algorithm.training.scheduler_params.sigma=0.5 are the hyperparameters for the gaussian scheduler. If you use other schedulers (like this cosine scheduler), you can ignore these two parameters. Note that it will affect the experiment run name.
- The model will be uploaded to my huggingface model hub, the running name is determined by methods/RL/conf/config.yaml (output.run_name: ... [you need to check the config.yaml]).

### Inference
```bash
CUDA_VISIBLE_DEVICES=5 ROOT_PATH=/data/shurui.gui/Projects/Sys2Bench python methods/RL/main.py mode=inference task=countdown2345
 algorithm=grpo model=qwen15 model.family=citrinegui model.trim=Qwen2.5-1.5B-Instruct_countdown345_grpo_balanced_0.5_0.5_True_1600 
 task.test_file=citrinegui/countdown_n6t100_1-100 algorithm.training.max_steps=1600 task.inference.batch_size=32
```
- model.family=citrinegui: the model and datasets are saved in my model hub.
- model.trim: the model name in the model hub, which is `Qwen2.5-1.5B-Instruct_countdown2345_grpo_cosine_0.5_0.5_True_1600`, this name is automatically generated by the abovementioned training process, so you can see the hyperparameters to running name relation. 
- task.test_file=citrinegui/countdown_n6t100_1-100: Test on the countdown task with 6 numbers. For the dataset name, check the data generation process in methods/RL/data_gen/.

## Architecture

The implementation centers around the `TaskSampler` class (lines 56-131 in main.py) which manages curriculum learning:

```python
class TaskSampler(torch.utils.data.Sampler):
    def __init__(self, dataset, num_tasks, total_iterations, 
                 data_schedule, batch_size, scheduler_params, seed=0)
```

The sampler:
1. Organizes data by difficulty level (task 0 = easiest, task N-1 = hardest)
2. Applies the chosen scheduling function at each iteration
3. Samples tasks according to the computed probabilities

## Curriculum Schedulers

The implementation includes five scheduling strategies:

### 1. **Balanced Schedule** (`balanced`)
- Equal probability for all difficulty levels throughout training
- P(task_i) = 1/num_tasks

### 2. **Classical Curriculum** (`classic`)
- Sequential progression through difficulties
- Trains on task i for T/num_tasks iterations before moving to task i+1

### 3. **Cosine Schedule** (`cosine`)
- Smooth transition from easy to hard tasks
- Uses cosine annealing to shift probabilities
- Includes minimum probability floor to prevent task starvation

### 4. **Gaussian Schedule** (`gaussian`)
- Bell curve that moves from easy to hard tasks
- Configurable parameters:
  - `mu_exp`: Controls progression speed (default: 1.0)
  - `sigma`: Standard deviation of the Gaussian
  - `min_prob`: Minimum probability per task

### 5. **Variance Regularized** (`variance_regularized`) [Experimental: still exploring, your focus]
- Adaptive scheduling based on performance variance
- Imported from external module
- Updates probabilities based on reward feedback

## Supported Tasks

The implementation supports four main task categories:

1. **BlocksWorld** - Planning task with block manipulation
2. **Countdown** - Arithmetic puzzle solving (Main focus, please do experiments on it)
3. **Arithmetic** - GSM8K, AQuA, MATH datasets
4. **Coding** - Programming problem solving

Each task has multiple difficulty levels defined by separate data files.

## Configuration

The system uses Hydra for configuration management. Check methods/RL/conf/ thoroughly.

## Monitoring Training
[To be filled] Since I use wandb to monitor the training, not sure whether you can also do that? Or it is better to log in your way? If you want to log in your way, modify the code and add instructions here later.

### Key Implementation Details

1. **Prompt Format**: All tasks use a consistent format with `<think>` tags for reasoning and task-specific answer tags
2. **Reward Functions**: Each task has a custom reward function that validates format and correctness
3. **Batch Sampling**: Tasks are sampled per-batch based on scheduler probabilities
4. **Data Exhaustion**: When a difficulty level's data is exhausted, it's reshuffled

## ðŸš¨ CRITICAL DEVELOPMENT PRINCIPLE

**NEVER ASSUME DATA STRUCTURES - ALWAYS PRINT AND VERIFY**
- Print type and content of EVERY new variable: `print(f"var type: {type(var)}, content: {var}")`
- Test EACH line of new code immediately - treat every line as a potential failure point
- Silent failures cascade into debugging nightmares - add assertions and explicit checks

## ðŸ’¡ Debugging Tips

- **Speed up data preparation**: Add `task.train_size=1000` to reduce dataset mapping time from 3 minutes to seconds

## ðŸ“‹ Important Notes

- **Entry Point**: Always use `methods/RL/main.py` as the entry point
- **Scheduler Selection**: Choose scheduler based on task characteristics
- **GPU Memory**: Configure batch size and gradient accumulation based on available memory
- **Checkpoint Saving**: Models are saved at regular intervals defined in configuration